{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ff210b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re, string\n",
    "import emoji\n",
    "import nltk\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import RobertaTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0807be3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r'C:/Users/Admin/Desktop/Coronavirus Tweets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "108b62ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName   Location     TweetAt  \\\n",
       "0      3799       48751     London  16-03-2020   \n",
       "1      3800       48752         UK  16-03-2020   \n",
       "2      3801       48753  Vagabonds  16-03-2020   \n",
       "3      3802       48754        NaN  16-03-2020   \n",
       "4      3803       48755        NaN  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
       "1  advice Talk to your neighbours family to excha...            Positive  \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive  \n",
       "3  My food stock is not the only one which is emp...            Positive  \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(base_path + '/Data/Corona_NLP_train.csv', encoding='ISO-8859-1')\n",
    "df_test = pd.read_csv(base_path + '/Data/Corona_NLP_test.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ccbf2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41157 entries, 0 to 41156\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   UserName       41157 non-null  int64 \n",
      " 1   ScreenName     41157 non-null  int64 \n",
      " 2   Location       32567 non-null  object\n",
      " 3   TweetAt        41157 non-null  object\n",
      " 4   OriginalTweet  41157 non-null  object\n",
      " 5   Sentiment      41157 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f232667",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['OriginalTweet','Sentiment']]\n",
    "df_test = df_test[['OriginalTweet','Sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64a6a21",
   "metadata": {},
   "source": [
    "## Defind functions to clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04af114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove emojis\n",
    "def strip_emoji(text):\n",
    "    \n",
    "    return re.sub(emoji.get_emoji_regexp(), r\"\", text)\n",
    "\n",
    "\n",
    "# remove punctuations, links, mentions and \\r\\n new line characters\n",
    "def strip_all_entities(text):\n",
    "    # remove \\n and \\r and lowercase\n",
    "    text = text.replace('\\r', '').replace('\\n', ' ').replace('\\n', ' ').lower() \n",
    "    \n",
    "    # remove links and mentions\n",
    "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text) \n",
    "    \n",
    "    # remove non utf8/ascii characters such as '\\x9a\\x91\\x97\\x9a\\x97'\n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r'', text)\n",
    "    \n",
    "    banned_list= string.punctuation + 'Ã'+'±'+'ã'+'¼'+'â'+'»'+'§'\n",
    "    table = str.maketrans('', '', banned_list)\n",
    "    text = text.translate(table)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "#clean hashtags\n",
    "def clean_hashtags(tweet):\n",
    "    new_tweet = \" \".join(word.strip() for word in re.split('#(?!(?:hashtag)\\b)[\\w-]+(?=(?:\\s+#[\\w-]+)*\\s*$)', tweet)) \n",
    "    new_tweet2 = \" \".join(word.strip() for word in re.split('#|_', new_tweet))\n",
    "    \n",
    "    return new_tweet2\n",
    "\n",
    "\n",
    "# filter special characters such as & and $ present in some words\n",
    "def filter_chars(a):\n",
    "    sent = []\n",
    "    for word in a.split(' '):\n",
    "        if ('$' in word) | ('&' in word):\n",
    "            sent.append('')\n",
    "        else:\n",
    "            sent.append(word)\n",
    "            \n",
    "    return ' '.join(sent)\n",
    "\n",
    "\n",
    "# remove multiple spaces\n",
    "def remove_mult_spaces(text): \n",
    "    \n",
    "    return re.sub(\"\\s\\s+\" , \" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0b837d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords & Punctuations\n",
    "from nltk.corpus import stopwords\n",
    "\", \".join(stopwords.words('english'))\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def punct_remove(text):\n",
    "    punct = re.sub(r\"[^\\w\\s\\d]\",\"\", text)\n",
    "    \n",
    "    return punct\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f023b26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to correct misspellings and out of vocab words\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "\n",
    "mispell_dict = {'colour':'color',\n",
    "                'centre':'center',\n",
    "                'favourite':'favorite',\n",
    "                'travelling':'traveling',\n",
    "                'counselling':'counseling',\n",
    "                'theatre':'theater',\n",
    "                'cancelled':'canceled',\n",
    "                'labour':'labor',\n",
    "                'organisation':'organization',\n",
    "                'wwii':'world war 2',\n",
    "                'citicise':'criticize',\n",
    "                'instagram': 'social medium',\n",
    "                'whatsapp': 'social medium',\n",
    "                'snapchat': 'social medium',\n",
    "                'Snapchat': 'social medium',\n",
    "                'quora': 'social medium',\n",
    "                'Quora': 'social medium',\n",
    "                'mediumns': 'mediums',\n",
    "                'bitcoin': 'currency',\n",
    "                'cryptocurrency': 'currency',\n",
    "                'upsc': 'union public service commission',\n",
    "                'mbbs': 'bachelor medicine',\n",
    "                'ece': 'educational credential evaluators',\n",
    "                'aiims': 'all india institute medical science',\n",
    "                'iim': 'india institute management',\n",
    "                'sbi': 'state bank india',\n",
    "                'blockchain': 'crytography',\n",
    "                'and': '',\n",
    "                'reducational':'educational',\n",
    "                'neducational':'educational',\n",
    "                'greeducational': 'greed educational',\n",
    "                'pieducational': 'educational',\n",
    "                'deducational': 'educational',\n",
    "                'Quorans': 'Quoran',\n",
    "                'stayhome': 'stay at home',\n",
    "                'stayathome': 'stay at home',\n",
    "                'toiletpaper': 'toilet paper',\n",
    "                'covid_': 'covid',\n",
    "                'pemic': 'pandemic',\n",
    "                'panicbuying': 'panic buying',\n",
    "                'quarantinelife': 'quarantine life',\n",
    "                'coronacrisis': 'corona crisis',\n",
    "                'underst': 'underestimate',\n",
    "                'hsanitizer': 'sanitizer',\n",
    "                'asda': '',\n",
    "                'realdonaldtrump': 'real donald trump',\n",
    "                'coronavirusuk': 'virus',\n",
    "                'evaluatorsssion': 'evaluate',\n",
    "                'evaluatorsdented': 'evaluate',\n",
    "                'unpeducational': 'no educational',\n",
    "                'pemic': 'pandemic',\n",
    "                'coronavirususa': 'virus',\n",
    "                'coronavirus': 'virus',\n",
    "                'buyinguk': 'buy',\n",
    "                'stoppanic': 'stop panic',\n",
    "                'staysafe': 'stay safe',\n",
    "                'gov': 'government'\n",
    "                }\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    text = text.lower()\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "\n",
    "    return mispellings_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "567be725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean train data\n",
    "texts_new = []\n",
    "for t in df.OriginalTweet:\n",
    "    texts_new.append(remove_mult_spaces(filter_chars(clean_hashtags(strip_all_entities(strip_emoji(t))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25313a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean test data\n",
    "texts_new_test = []\n",
    "for t in df_test.OriginalTweet:\n",
    "    texts_new_test.append(remove_mult_spaces(filter_chars(clean_hashtags(strip_all_entities(strip_emoji(t))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f566f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_clean'] = texts_new\n",
    "df_test['text_clean'] = texts_new_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9b1bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_clean'] = df['text_clean'].apply(lambda x : punct_remove(x))\n",
    "df['text_clean'] = df['text_clean'].apply(lambda x : remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fb181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_clean'] = df['text_clean'].apply(lambda x : replace_typical_misspell(x))\n",
    "df_test['text_clean'] = df_test['text_clean'].apply(lambda x : replace_typical_misspell(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9282ec20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_clean'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefcc651",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['text_clean'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a676f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the length of text in train data\n",
    "text_len = []\n",
    "for text in df.text_clean:\n",
    "    tweet_len = len(text.split())\n",
    "    text_len.append(tweet_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0a8020",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_len'] = text_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91359b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the length of text in test data\n",
    "text_len_test = []\n",
    "for text in df_test.text_clean:\n",
    "    tweet_len = len(text.split())\n",
    "    text_len_test.append(tweet_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf69a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['text_len'] = text_len_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2551e68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" df shape: {df.shape}\")\n",
    "print(f\" df_test shape: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a6eac2",
   "metadata": {},
   "source": [
    "## Remove Tweets that have length lesser than 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0fd0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['text_len'] > 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7e2e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test[df_test['text_len'] > 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc7c610",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" df shape: {df.shape}\")\n",
    "print(f\" df_test shape: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf38ec36",
   "metadata": {},
   "source": [
    "## BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49df3aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85553496",
   "metadata": {},
   "source": [
    "## Find tweets that are not in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20400283",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lens = []\n",
    "\n",
    "for txt in df['text_clean'].values:\n",
    "    tokens = tokenizer.encode(txt, max_length = 512, truncation = True)\n",
    "    token_lens.append(len(tokens))\n",
    "    \n",
    "max_len = np.max(token_lens)\n",
    "print(f\"Max tokenized sentence length: {max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b81861",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lens = []\n",
    "\n",
    "for i,txt in enumerate(df['text_clean'].values):\n",
    "    tokens = tokenizer.encode(txt, max_length = 512, truncation = True)\n",
    "    token_lens.append(len(tokens))\n",
    "    if len(tokens) > 80:\n",
    "        print(f\"Index: {i}, Text: {txt}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9757486",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['token_lens'] = token_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a64cf72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.sort_values(by='token_lens', ascending=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d0e8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[12:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09d2c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede3d58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lens_test = []\n",
    "\n",
    "for txt in df_test['text_clean'].values:\n",
    "    tokens = tokenizer.encode(txt, max_length = 512, truncation = True)\n",
    "    token_lens_test.append(len(tokens))\n",
    "    \n",
    "max_len=np.max(token_lens_test)\n",
    "print(f\"Max tokenized sentence length: {max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520e391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lens_test = []\n",
    "\n",
    "for i,txt in enumerate(df_test['text_clean'].values):\n",
    "    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
    "    token_lens_test.append(len(tokens))\n",
    "    if len(tokens)>80:\n",
    "        print(f\"Index: {i}, Text: {txt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad70e49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['token_lens'] = token_lens_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3da47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.sort_values(by='token_lens', ascending=False)\n",
    "df_test.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d9929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[5:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459f3282",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.sample(frac=1).reset_index(drop=True)\n",
    "df_test.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7882d79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9f34fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentiment'] = df['Sentiment'].map({'Extremely Negative':0,'Negative':0,'Neutral':1,'Positive':2,'Extremely Positive':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879a6e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Sentiment'] = df_test['Sentiment'].map({'Extremely Negative':0,'Negative':0,'Neutral':1,'Positive':2,'Extremely Positive':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bc89e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddb51c7",
   "metadata": {},
   "source": [
    "## Random Over Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd5c9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "ros = RandomOverSampler()\n",
    "train_x, train_y = ros.fit_resample(np.array(df['text_clean']).reshape(-1, 1), \n",
    "                                    np.array(df['Sentiment']).reshape(-1, 1))\n",
    "\n",
    "\n",
    "train_data = pd.DataFrame(list(zip([x[0] for x in train_x], train_y)), \n",
    "                        columns = ['text_clean', 'Sentiment']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20f0f59",
   "metadata": {},
   "source": [
    "# Train - Validation - Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552d8a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data['text_clean'].values\n",
    "y = train_data['Sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e75e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dab190",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test['text_clean'].values\n",
    "y_test = df_test['Sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f75124",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7667f6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./Clean data with 2 layers/X_train.npy', X_train)\n",
    "np.save('./Clean data with 2 layers/y_train.npy', y_train)\n",
    "np.save('./Clean data with 2 layers/X_valid.npy', X_valid)\n",
    "np.save('./Clean data with 2 layers/y_valid.npy', y_valid)\n",
    "np.save('./Clean data with 2 layers/X_test.npy', X_test)\n",
    "np.save('./Clean data with 2 layers/y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd262c57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
